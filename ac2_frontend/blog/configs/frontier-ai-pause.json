{
    "slug": "ai-pause",
    "title": "Pausing Frontier AI Development",
    "date": "November 22, 2025",
    "excerpt": "Frontier AI labs, such as OpenAI, Anthropic, and Google, have been racing to develop the most powerful AI models. However, these models are increasingly dangerous and should be paused until they are safer.",
    "description": "Frontier AI labs have been racing to develop powerful AI models, and have publicly stated that development should be paused until we can guarantee their safety.",
    "content": "<h2>Racing For Powerful AI</h2><p>Companies developing advanced AI models are now some of the most <a href=\"https://www.reuters.com/technology/openai-hits-500-billion-valuation-after-share-sale-source-says-2025-10-02/\">valuable on the planet</a>. They are building a world-changing technology which, by the admission of some of the people who have founded these AI companies, have the potential to cause <a href=\"https://www.cbsnews.com/news/anthropic-ceo-dario-amodei-warning-of-ai-potential-dangers-60-minutes-transcript/\">immense harm to humans</a> and potentially end the world as we know it. So if this is so dangerous, how do they justify continuing to build this?</p>\n\n<h2>No-one Can Stop Unless Everyone Stops</h2>\n<p>They claim that thye cannot stop unless everyone stops. We are locked in a race, the race to create a machine smarter than a human, and whoever can do that first gets to decide how it is built and what it does to the humans it leaves behind. These tech CEOs and investors say that we are in a <a href=\"https://www.forbes.com/councils/forbestechcouncil/2025/07/28/the-race-for-ai-supremacy-a-new-frontier-of-international-competition/\">race to build superintelligence</a> and that it would be irresponsible to pause unless everyone else did so as well.</p>\n\n<h2>Credibly Committing To A Pause</h2>\n<p>Using AC2, we can allow these AI labs to credibly commit to a pause even without publicly stating that they have made this commitment. A legally binding treaty can be put together, such that all of the AI labs guarantee that they will not release any more powerful models for a year, if the other labs also agree to this. Labs can then sign up to this without publishing that they have done so, reducing the chance that they get investment restrictions and issues hiring, with the agreement coming into effect when enough labs anonymously sign up.</p>\n\n<h2>Building Responsible Regulation</h2>\n<p>Allowing industries to self-regulate in this manner, halting their activities when their own technology cannot be made safe without further research, lets us combine the speed of private industry with the responsibility of collective action.</p>"
  }
  
  